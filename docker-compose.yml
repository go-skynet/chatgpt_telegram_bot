version: "3"

services:
  api:
    image: quay.io/go-skynet/local-ai:v1.24.1-ffmpeg
    # As initially LocalAI will download the models defined in PRELOAD_MODELS
    # you might need to tweak the healthcheck values here according to your network connection.
    # Here we give a timespan of 20m to download all the required files.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 20
    ports:
      - 8080:8080
    environment:
      - DEBUG=true
      - MODELS_PATH=/models
      - IMAGE_PATH=/tmp
      - 'PRELOAD_MODELS_CONFIG=/config/localai.yml'
    volumes:
      - ./config:/config:cached
      - ./models:/models:cached
    command: ["/usr/bin/local-ai" ]
 
  chatgpt_telegram_bot:
    container_name: chatgpt_telegram_bot
    restart: always
    environment:
      - OPENAI_API_KEY=sk---anystringhere
      - OPENAI_API_BASE=http://api:8080/v1
    build:
      context: "."
      dockerfile: Dockerfile
    depends_on:
      api:
        condition: service_healthy
